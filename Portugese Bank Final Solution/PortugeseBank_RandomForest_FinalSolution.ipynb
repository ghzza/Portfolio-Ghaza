{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote_notebook_id": "af2783aa-5974-457c-a1bb-3cb2b2fcc2d9",
    "deepnote": {
      "is_reactive": false
    },
    "deepnote_execution_queue": [],
    "colab": {
      "name": "PortugeseBank_RandomForest_FinalSolution.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6y8q6RiX6Neg",
        "gsxP21gR6Neh",
        "5Xc4hAVt6Nej"
      ],
      "toc_visible": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00000-d3d08d12-0a66-40fc-a445-01afa71ebff6",
        "deepnote_cell_type": "text-cell-h1",
        "id": "71XlOSME6Ndj"
      },
      "source": [
        "# Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "73efdfa6",
        "execution_millis": 5377,
        "cell_id": "00001-32485321-38e2-49a9-a785-00621ec313cf",
        "execution_start": 1618510389196,
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6HCzPn36Nd4",
        "outputId": "1afb262f-a860-47a5-e027-80bcaf68048d"
      },
      "source": [
        "!pip install imblearn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (from imblearn) (0.4.3)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->imbalanced-learn->imblearn) (1.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "a40e8223",
        "execution_millis": 1900,
        "cell_id": "00002-ff121aa0-8971-4e50-81a3-2767f48acdcd",
        "execution_start": 1618510394591,
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KKCV29s6Nd9",
        "outputId": "7b34c762-c3af-4cf7-8305-c89285b5647d"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00003-21729fd5-c5b2-466b-88d4-30eeb9f38243",
        "deepnote_cell_type": "text-cell-h1",
        "id": "rDAzT0q16NeA"
      },
      "source": [
        "# Read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "5285587f",
        "execution_millis": 283,
        "cell_id": "00004-1e646a86-75ee-4e7e-bc03-4ba2892d5441",
        "execution_start": 1618510396497,
        "deepnote_cell_type": "code",
        "id": "OI7AY64L6NeB"
      },
      "source": [
        "data = pd.read_csv(\"bank-full.csv\", delimiter=';')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00005-c9207795-0512-40e4-8007-21be81f13732",
        "deepnote_cell_type": "text-cell-h1",
        "id": "FYzbZzFX6NeC"
      },
      "source": [
        "# Simple Data Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "2007f58",
        "execution_millis": 18,
        "cell_id": "00006-7b90ca9b-3b24-48e6-8062-e7dec68ea2a8",
        "execution_start": 1618510396784,
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOZK75PY6NeD",
        "outputId": "2fb8fa74-72aa-4512-b192-0a50dbf477de"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(45211, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00007-a7ddcf51-3f29-4ed3-861d-cac890ee804c",
        "deepnote_cell_type": "text-cell-h1",
        "id": "CiCFIa-w6NeE"
      },
      "source": [
        "# Model Construction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00008-e1b7e10c-0d92-4b20-a009-573fab354b12",
        "deepnote_cell_type": "text-cell-h2",
        "id": "cFIQmhmg6NeF"
      },
      "source": [
        "## Label Encoding For Categorical Data Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00009-f290bb47-1e9d-4113-81a1-300c7389bbb6",
        "deepnote_cell_type": "text-cell-p",
        "id": "62hgkN3r6NeG"
      },
      "source": [
        "Label Encoding are used to transform non-numerical&nbsp;labels&nbsp;(as long as they are hashable and comparable) to numerical&nbsp;labels. Label that are being transform to numerical are categorical data column which is job, marital, education, default, housing, loan, contact, month and poutcome."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "86f54dab",
        "execution_millis": 226,
        "cell_id": "00010-2e26410f-7ea2-4026-acda-02bf355e2b47",
        "execution_start": 1618510396805,
        "deepnote_cell_type": "code",
        "id": "kvdhm1ji6NeH"
      },
      "source": [
        "le = LabelEncoder()\n",
        "encode_x = data.iloc[ : , :-1]\n",
        "encode_x.job = le.fit_transform(encode_x.job)\n",
        "encode_x.marital = le.fit_transform(encode_x.marital)\n",
        "encode_x.education = le.fit_transform(encode_x.education)\n",
        "encode_x.default = le.fit_transform(encode_x.default)\n",
        "encode_x.housing = le.fit_transform(encode_x.housing)\n",
        "encode_x.loan = le.fit_transform(encode_x.loan)\n",
        "encode_x.contact = le.fit_transform(encode_x.contact)\n",
        "encode_x.month = le.fit_transform(encode_x.month)\n",
        "encode_x.poutcome = le.fit_transform(encode_x.poutcome)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00011-3f6e03bf-4570-4651-9bec-9fc71272dcc9",
        "deepnote_cell_type": "text-cell-h2",
        "id": "XR37djMa6NeJ"
      },
      "source": [
        "## Split Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00012-5fc88d0b-68cb-4375-8043-eb7eb471a83d",
        "deepnote_cell_type": "text-cell-p",
        "id": "ekNpzaq66NeJ"
      },
      "source": [
        "Split data are used to split the original data into 2 different parts which is training data and testing data. Training data will also be divided into 2 parts training data and validation training data. This process to ensure when the model being trained we have a data to validate the predicted output of a model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "de325735",
        "execution_millis": 11,
        "cell_id": "00013-20106a6d-5802-4f04-b21a-1b1042579d30",
        "execution_start": 1618510397059,
        "deepnote_cell_type": "code",
        "id": "_aWKvUXf6NeK"
      },
      "source": [
        "split_input_data = encode_x\n",
        "split_output_data = data['y']"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00014-1195366e-c6f8-4c1f-82c4-7cdb6663b992",
        "deepnote_cell_type": "text-cell-p",
        "id": "KEgdT7ZL6NeL"
      },
      "source": [
        "We divided the data into 4 parts:\n",
        "- X_train is a training data that have no output.\n",
        "- X_test is a testing data that have no output. This data will be use by the model to predict the output of the X_test data\n",
        "- y_train is a training data that has the output data from X_train data.\n",
        "- y_test is the original output data from X_test. This data will be used to validate the accuracy of predicted data that were generated by the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "5e746e28",
        "execution_millis": 34,
        "cell_id": "00015-2cfa5e1a-4b3a-421f-8797-d50180b9dda4",
        "execution_start": 1618510397073,
        "deepnote_cell_type": "code",
        "id": "wG6oT7xe6NeM"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(split_input_data, split_output_data, test_size=0.3, random_state=10)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00016-77dc5c5c-17cf-4124-a8c4-4f4ad2376940",
        "deepnote_cell_type": "text-cell-h2",
        "id": "ZH-ZuFrS6NeN"
      },
      "source": [
        "## Scale Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00017-fd22bb55-1e57-4700-9aaf-c02d005b88d0",
        "deepnote_cell_type": "text-cell-p",
        "id": "_zCJkrp86NeN"
      },
      "source": [
        "Here we transform the data to fit within a specific scale using these algorithms a change of \"1\" in any numeric feature will give the same importance to each data. We used StandardScaler() method from Sklearn library. Define the transformation for train and test data:\n",
        "- X_train will be scale using fit_transform()\n",
        "- X_test will be scale using transform()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "7ca32dca",
        "execution_millis": 55,
        "cell_id": "00018-5c948e53-ca25-44b6-9964-5d93c52d8bd0",
        "execution_start": 1618510397112,
        "deepnote_cell_type": "code",
        "id": "TozDiqps6NeO"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train))\n",
        "X_test_scaled = pd.DataFrame(scaler.transform(X_test))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00019-8a492faa-d46a-446e-841c-ef0b0ccf3bf4",
        "deepnote_cell_type": "text-cell-h2",
        "id": "bEEMC8bC6NeP"
      },
      "source": [
        "## Mix Sampling using SMOTE &amp; Random Under Sampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00020-27d5108a-87a5-4e82-a361-594fd3501dda",
        "deepnote_cell_type": "text-cell-p",
        "id": "sPliL_UF6NeQ"
      },
      "source": [
        "Here we distributed the data using SMOTE and Random Under Sampler to make the classification output data balance. We used SMOTE to duplicates and variance the 'yes' data for the model to learn more variance of yes data.  We used Random Under Sampler to delete random sample of the 'no' data to make the model not to have low bias towards 'yes' data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "aa80fcd1",
        "execution_millis": 1212,
        "cell_id": "00021-802b893b-6a70-41a5-b2ca-a1ee2610bc3f",
        "execution_start": 1618510397205,
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRlx9EzU6NeR",
        "outputId": "4241b316-e7e7-491c-bc17-ca0ecc79cd7e"
      },
      "source": [
        "mixSample_X = X_train_scaled\n",
        "# define pipeline\n",
        "over = SMOTE(sampling_strategy=0.15)\n",
        "under = RandomUnderSampler(sampling_strategy=0.5)\n",
        "steps = [('o', over), ('u', under)]\n",
        "pipeline = Pipeline(steps=steps)\n",
        "# transform the dataset\n",
        "mixSample_X, mixSample_Y = pipeline.fit_resample(mixSample_X, y_train)\n",
        "pd.DataFrame(mixSample_Y).value_counts()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "no     8374\n",
              "yes    4187\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00022-d8628064-9767-4f54-b5d3-591763537791",
        "deepnote_cell_type": "text-cell-h1",
        "id": "Bs5YJQIv6NeS"
      },
      "source": [
        "# Random Forest from SKLEARN Library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00023-4a3e65a8-e224-4f81-b441-ec336b87f3b5",
        "deepnote_cell_type": "text-cell-h2",
        "id": "-_OeFYi16NeU"
      },
      "source": [
        "## Model Fit with default Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00024-cf70e74b-4d16-475e-bf1b-6bacd96d1a58",
        "deepnote_cell_type": "text-cell-p",
        "id": "xIjw5SD16NeU"
      },
      "source": [
        "Here we used Random Forest only define class weight parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "3f8d68e",
        "execution_millis": 2,
        "cell_id": "00025-cff5aa33-1427-4382-a143-f67869fefe03",
        "execution_start": 1618510398422,
        "deepnote_cell_type": "code",
        "id": "klKS2BOO6NeV"
      },
      "source": [
        "model = RandomForestClassifier()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00026-e4cce382-cef3-4ba3-82a9-87d0faca8ccb",
        "deepnote_cell_type": "text-cell-h2",
        "id": "duhBww086NeX"
      },
      "source": [
        "## Train the Model using Training Data and Validation Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "21cd9296",
        "execution_millis": 4332,
        "cell_id": "00027-16136058-3c35-4ecb-84d3-bf2606b02787",
        "execution_start": 1618510398427,
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5nXhSAB6NeY",
        "outputId": "8cefdd71-c997-42e6-f1cc-dae3cf173ce0"
      },
      "source": [
        "model.fit(mixSample_X, mixSample_Y)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00028-12b8c02c-7646-413e-b33b-8cc5c283c6b6",
        "deepnote_cell_type": "text-cell-h2",
        "id": "Tm9vNBd26NeZ"
      },
      "source": [
        "## Predict Model using Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "48992fe",
        "execution_millis": 419,
        "cell_id": "00029-10a3d0d2-99ba-4486-890e-871d863dbe66",
        "execution_start": 1618510402754,
        "deepnote_cell_type": "code",
        "id": "idVDjArw6Nea"
      },
      "source": [
        "y_pred = model.predict(X_test_scaled)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00030-abbfdb5a-77d0-4c67-99a4-df48ff766727",
        "deepnote_cell_type": "text-cell-h3",
        "id": "-1lslr6o6Nea"
      },
      "source": [
        "### Before evaluation we need to know the value count of each class from the original output data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00031-36b2df20-ba8e-4891-bfab-c24a2afc5459",
        "deepnote_cell_type": "text-cell-p",
        "id": "hALj6iik6Neb"
      },
      "source": [
        "Here are the count of 'no' data and the count of 'yes' data from the original output data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "9f353983",
        "execution_millis": 1,
        "cell_id": "00032-919d0009-cc8a-4fef-be11-dd132348885d",
        "execution_start": 1618510403186,
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sv9iNNlI6Neb",
        "outputId": "1833e0a5-cedc-4ef9-d30e-58697e41294a"
      },
      "source": [
        "y_test.value_counts()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "no     12006\n",
              "yes     1558\n",
              "Name: y, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00033-04ffc0a3-0b19-4aed-ba04-95a96a792d5c",
        "deepnote_cell_type": "text-cell-h2",
        "id": "aeSZOhFH6Nec"
      },
      "source": [
        "## Evaluation Metrics using Confusion Matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00034-ca81310d-642f-410d-bb07-6065c7c7d63b",
        "deepnote_cell_type": "text-cell-p",
        "id": "Lt9B1E4Y6Ned"
      },
      "source": [
        "Here we used confusion matrix which is crosstab() method from pandas There are 4 terms as a representation of the result of the classification process confusion matrix. The four terms: \n",
        "- True Positive (TP): Represents positive data that is predicted to be correct.\n",
        "- True Negative (TN): Represents negative data that is predicted to be correct.\n",
        "- False Positive (FP) Type I Error: Represents negative data but predicted as positive data.\n",
        "- False Negative (FN) Type II Error: Represents positive data but predicted as negative data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "834f5afa",
        "execution_millis": 34,
        "cell_id": "00035-68b05e7f-9084-4a17-bf6d-edc0418139ae",
        "execution_start": 1618510403200,
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0VTsKeW6Nee",
        "outputId": "d9717159-6322-4373-8fbf-5eb45f3af922"
      },
      "source": [
        "confusion_matrix = pd.crosstab(y_test, y_pred)\n",
        "print (confusion_matrix)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "col_0     no   yes\n",
            "y                 \n",
            "no     10732  1274\n",
            "yes      359  1199\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00036-95c7dba0-de11-4ad8-ae38-36b2c5a17b20",
        "deepnote_cell_type": "text-cell-p",
        "id": "1kdUdCu56Nee"
      },
      "source": [
        " The conclusion from the above result: \n",
        "- True Positive (TP): **1199** 'Yes' predicted data is correct **from 1558** 'Yes' original data \n",
        "- True Negative (TN): **10732** 'No' predicted data is correct **from 12006** 'No' original data\n",
        "- False Negative (FN): **359** 'Yes' predicted data falsely predicted as 'No' data\n",
        "- False Positive (FP): **1274** 'No' predicted data falsely predicted as 'Yes' data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00037-c0384c28-544a-4186-96bd-41e38dbc6cab",
        "deepnote_cell_type": "text-cell-h2",
        "id": "HVUdc3dm6Nef"
      },
      "source": [
        "## Evaluation Metrics using Classification Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00038-f8a68410-b1e0-4bba-996b-5582cf82c595",
        "deepnote_cell_type": "text-cell-p",
        "id": "Nais13ZN6Neg"
      },
      "source": [
        "Here we used the classification report from sklearn library in the classification report function we have precision, recall, f1-score and support for the evaluation metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00039-5c545dce-2abd-4cf1-9ff6-aa6df6820b26",
        "deepnote_cell_type": "text-cell-h3",
        "id": "6y8q6RiX6Neg"
      },
      "source": [
        "### Precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00040-e52cf22d-058d-4beb-bb93-fa4aab12f054",
        "deepnote_cell_type": "text-cell-p",
        "id": "Hb0L5am26Neh"
      },
      "source": [
        "Precision is the ability of a classifier not to label an instance positive that is actually negative. For each class, it is defined as the ratio of true positives to the sum of a true positive and false positive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00041-655caed2-319c-4909-b79d-d7e8a8add61b",
        "deepnote_cell_type": "text-cell-h3",
        "id": "gsxP21gR6Neh"
      },
      "source": [
        "### Recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00042-dceaa23c-1e04-45a0-9691-ca45a58923df",
        "deepnote_cell_type": "text-cell-p",
        "id": "RL21LtD76Nei"
      },
      "source": [
        "Recall is the ability of a classifier to find all positive instances. For each class it is defined as the ratio of true positives to the sum of true positives and false negatives."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00043-ae5279c6-1f7c-40e9-88f2-ce63a69c9d94",
        "deepnote_cell_type": "text-cell-h3",
        "id": "5Xc4hAVt6Nej"
      },
      "source": [
        "### F1-Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00044-7d0e1e0e-3d6a-4a8a-a8bd-adae247f8a28",
        "deepnote_cell_type": "text-cell-p",
        "id": "HDi68VJ_6Nej"
      },
      "source": [
        "The F1 score is a weighted harmonic mean of precision and recall such that the best score is 1.0 and the worst is 0.0. F1 scores are lower than accuracy measures as they embed precision and recall into their computation. As a rule of thumb, the weighted average of F1 should be used to compare classifier models, not global accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00045-71508676-4b45-4d05-8dd1-06c04865ae01",
        "deepnote_cell_type": "text-cell-h3",
        "id": "COiT0T9s6Nek"
      },
      "source": [
        "### Support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00046-196ac055-0cd8-4ca9-ae91-0a51053993f6",
        "deepnote_cell_type": "text-cell-p",
        "id": "PVwNKsgj6Nek"
      },
      "source": [
        "Support is the number of actual occurrences of the class in the specified dataset. Imbalanced support in the training data may indicate structural weaknesses in the reported scores of the classifier and could indicate the need for stratified sampling or rebalancing. Support doesn’t change between models but instead diagnoses the evaluation process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "74950708",
        "execution_millis": 1192,
        "cell_id": "00047-d87cc0e3-c5e3-48c4-a586-03880303378b",
        "execution_start": 1618510403230,
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ak75hME_6Nel",
        "outputId": "5c722d79-f10a-4063-8f62-f425da8ca37b"
      },
      "source": [
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          no       0.97      0.89      0.93     12006\n",
            "         yes       0.48      0.77      0.59      1558\n",
            "\n",
            "    accuracy                           0.88     13564\n",
            "   macro avg       0.73      0.83      0.76     13564\n",
            "weighted avg       0.91      0.88      0.89     13564\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00048-6a836957-1288-4195-8d32-92e1c484628e",
        "deepnote_cell_type": "text-cell-p",
        "id": "iOiRWicX6Nem"
      },
      "source": [
        " The conclusion from the above result:\n",
        "- Precision: **97%** 'No' predicted data are correctly predict &amp; **48%** 'Yes' predicted data are correctly predict\n",
        "- Recall: **89%** 'No' original data have been predicted by the model &amp; **77%** 'Yes' original data have been predicted by the model\n",
        "- F1-Score: The mean from precision and recall for 'No' predicted data is **93%** and for 'Yes' predicted data is **59%**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00049-23c8c56b-eca0-4c44-adf5-7127cd60f44a",
        "deepnote_cell_type": "text-cell-h1",
        "id": "ve9VUIP86Nem"
      },
      "source": [
        "# Model Fit with Best Parameters using Grid Search CV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00050-bbe0f9cd-48c3-4daf-b7f2-773d751e0240",
        "deepnote_cell_type": "text-cell-p",
        "id": "wz8Z6vXb6Nen"
      },
      "source": [
        "We used Grid Search CV from to find best parameter for our Random Forest model here we will compare the model that use default parameter and the model that will be given the best parameter from Grid Search CV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00051-e4795172-dd82-42d0-9153-525b18a4a7c1",
        "deepnote_cell_type": "text-cell-h2",
        "id": "vDabnwkM6Nen"
      },
      "source": [
        "## Define Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "42c7e10d",
        "execution_millis": 8,
        "cell_id": "00052-2b700568-5775-49cb-9d90-8db121e68f64",
        "execution_start": 1618510404401,
        "deepnote_cell_type": "code",
        "id": "-JrbMrbV6Neo"
      },
      "source": [
        "parameters = {\n",
        "        'criterion':['gini','entropy'],\n",
        "        'bootstrap':[True,False],\n",
        "         'oob_score':[True,False],\n",
        "        'warm_start':[True,False],\n",
        "        'class_weight' : ['balanced', 'balanced_subsample']\n",
        "}"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00053-de40e07d-160e-4c80-bd70-8fb7b966058d",
        "deepnote_cell_type": "text-cell-h2",
        "id": "g1vnzS0r6Nep"
      },
      "source": [
        "## Find Best Parameter using Randomized Search CV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00054-9c544fec-b19e-4ba9-930e-e04f9a5a2909",
        "deepnote_cell_type": "text-cell-p",
        "id": "NPiYXMV96Nep"
      },
      "source": [
        "Here we want to focus the result based on the Recall evaluation metric because we would like to know if the result of the predicted output made by the model already predict most of the correct data inside the predicted output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "6a5edefe",
        "execution_millis": 279195,
        "cell_id": "00055-dc990f83-f2a1-4601-844f-9d26bc9b9fb6",
        "execution_start": 1618510404412,
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGfOQVxM6Neq",
        "outputId": "4f3a8eb1-5b85-44e9-f5e9-d455a4c01a56"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, make_scorer\n",
        "\n",
        "recall_scorer = make_scorer(recall_score,pos_label='yes')\n",
        "\n",
        "grid_model = RandomizedSearchCV(RandomForestClassifier(), param_distributions = parameters,n_iter=20,verbose=2,scoring=recall_scorer)\n",
        "\n",
        "grid_model.fit(mixSample_X, mixSample_Y)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "[CV] warm_start=False, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=False, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=True, total=   2.2s\n",
            "[CV] warm_start=False, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.2s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=False, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=True, total=   2.2s\n",
            "[CV] warm_start=False, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=True, total=   2.1s\n",
            "[CV] warm_start=False, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=True, total=   2.2s\n",
            "[CV] warm_start=False, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=True, total=   2.1s\n",
            "[CV] warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=False, total=   4.0s\n",
            "[CV] warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=False, total=   3.9s\n",
            "[CV] warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=False, total=   3.9s\n",
            "[CV] warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=False, total=   3.8s\n",
            "[CV] warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=False, total=   3.7s\n",
            "[CV] warm_start=True, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=True, total=   3.1s\n",
            "[CV] warm_start=True, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=True, total=   3.1s\n",
            "[CV] warm_start=True, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=True, total=   3.1s\n",
            "[CV] warm_start=True, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=True, total=   3.1s\n",
            "[CV] warm_start=True, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=True, total=   3.0s\n",
            "[CV] warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=True, total=   2.7s\n",
            "[CV] warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=True, total=   2.6s\n",
            "[CV] warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=True, total=   2.6s\n",
            "[CV] warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=True, total=   2.6s\n",
            "[CV] warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=True, total=   2.5s\n",
            "[CV] warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced, bootstrap=True, total=   2.8s\n",
            "[CV] warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced, bootstrap=True, total=   2.8s\n",
            "[CV] warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced, bootstrap=True, total=   2.8s\n",
            "[CV] warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced, bootstrap=True, total=   2.8s\n",
            "[CV] warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced, bootstrap=True, total=   2.7s\n",
            "[CV] warm_start=False, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=False \n",
            "[CV]  warm_start=False, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=False, total=   2.7s\n",
            "[CV] warm_start=False, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=False \n",
            "[CV]  warm_start=False, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=False, total=   2.7s\n",
            "[CV] warm_start=False, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=False \n",
            "[CV]  warm_start=False, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=False, total=   2.7s\n",
            "[CV] warm_start=False, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=False \n",
            "[CV]  warm_start=False, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=False, total=   2.7s\n",
            "[CV] warm_start=False, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=False \n",
            "[CV]  warm_start=False, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=False, total=   2.6s\n",
            "[CV] warm_start=True, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=True, total=   2.0s\n",
            "[CV] warm_start=True, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=True, total=   2.0s\n",
            "[CV] warm_start=True, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=True, total=   2.0s\n",
            "[CV] warm_start=True, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=True, total=   2.0s\n",
            "[CV] warm_start=True, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=True, total=   2.0s\n",
            "[CV] warm_start=True, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=False \n",
            "[CV]  warm_start=True, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=False, total=   0.0s\n",
            "[CV] warm_start=True, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=False \n",
            "[CV]  warm_start=True, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=False, total=   0.0s\n",
            "[CV] warm_start=True, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=False \n",
            "[CV]  warm_start=True, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=False, total=   0.0s\n",
            "[CV] warm_start=True, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=False \n",
            "[CV]  warm_start=True, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=False, total=   0.0s\n",
            "[CV] warm_start=True, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=False \n",
            "[CV]  warm_start=True, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=False, total=   0.0s\n",
            "[CV] warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=False \n",
            "[CV]  warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=False, total=   0.0s\n",
            "[CV] warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=False \n",
            "[CV]  warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=False, total=   0.0s\n",
            "[CV] warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=False \n",
            "[CV]  warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=False, total=   0.0s\n",
            "[CV] warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=False, total=   0.0s\n",
            "[CV] warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=False \n",
            "[CV]  warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=False, total=   0.0s\n",
            "[CV] warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced_subsample, bootstrap=True \n",
            "[CV]  warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced_subsample, bootstrap=True, total=   3.0s\n",
            "[CV] warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced_subsample, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced_subsample, bootstrap=True, total=   2.9s\n",
            "[CV] warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced_subsample, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced_subsample, bootstrap=True, total=   2.9s\n",
            "[CV] warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced_subsample, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced_subsample, bootstrap=True, total=   2.9s\n",
            "[CV] warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced_subsample, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=False, criterion=entropy, class_weight=balanced_subsample, bootstrap=True, total=   2.8s\n",
            "[CV] warm_start=False, oob_score=True, criterion=gini, class_weight=balanced_subsample, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=True, criterion=gini, class_weight=balanced_subsample, bootstrap=True, total=   2.3s\n",
            "[CV] warm_start=False, oob_score=True, criterion=gini, class_weight=balanced_subsample, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=True, criterion=gini, class_weight=balanced_subsample, bootstrap=True, total=   2.3s\n",
            "[CV] warm_start=False, oob_score=True, criterion=gini, class_weight=balanced_subsample, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=True, criterion=gini, class_weight=balanced_subsample, bootstrap=True, total=   2.3s\n",
            "[CV] warm_start=False, oob_score=True, criterion=gini, class_weight=balanced_subsample, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=True, criterion=gini, class_weight=balanced_subsample, bootstrap=True, total=   2.3s\n",
            "[CV] warm_start=False, oob_score=True, criterion=gini, class_weight=balanced_subsample, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=True, criterion=gini, class_weight=balanced_subsample, bootstrap=True, total=   2.2s\n",
            "[CV] warm_start=False, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=False \n",
            "[CV]  warm_start=False, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=False, total=   0.0s\n",
            "[CV] warm_start=False, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=False \n",
            "[CV]  warm_start=False, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=False, total=   0.0s\n",
            "[CV] warm_start=False, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=False \n",
            "[CV]  warm_start=False, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=False, total=   0.0s\n",
            "[CV] warm_start=False, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=False \n",
            "[CV]  warm_start=False, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=False, total=   0.0s\n",
            "[CV] warm_start=False, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=False \n",
            "[CV]  warm_start=False, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=False, total=   0.0s\n",
            "[CV] warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  FitFailedWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=True, total=   3.2s\n",
            "[CV] warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=True, total=   3.1s\n",
            "[CV] warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=True, total=   3.1s\n",
            "[CV] warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=True, total=   3.1s\n",
            "[CV] warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced_subsample, bootstrap=True, total=   3.0s\n",
            "[CV] warm_start=True, oob_score=True, criterion=gini, class_weight=balanced_subsample, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=True, criterion=gini, class_weight=balanced_subsample, bootstrap=True, total=   2.3s\n",
            "[CV] warm_start=True, oob_score=True, criterion=gini, class_weight=balanced_subsample, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=True, criterion=gini, class_weight=balanced_subsample, bootstrap=True, total=   2.3s\n",
            "[CV] warm_start=True, oob_score=True, criterion=gini, class_weight=balanced_subsample, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=True, criterion=gini, class_weight=balanced_subsample, bootstrap=True, total=   2.3s\n",
            "[CV] warm_start=True, oob_score=True, criterion=gini, class_weight=balanced_subsample, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=True, criterion=gini, class_weight=balanced_subsample, bootstrap=True, total=   2.3s\n",
            "[CV] warm_start=True, oob_score=True, criterion=gini, class_weight=balanced_subsample, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=True, criterion=gini, class_weight=balanced_subsample, bootstrap=True, total=   2.3s\n",
            "[CV] warm_start=False, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=True, total=   2.0s\n",
            "[CV] warm_start=False, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=True, total=   2.0s\n",
            "[CV] warm_start=False, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=True, total=   2.0s\n",
            "[CV] warm_start=False, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=True, total=   2.0s\n",
            "[CV] warm_start=False, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=True, total=   1.9s\n",
            "[CV] warm_start=True, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=True, total=   2.1s\n",
            "[CV] warm_start=True, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=True, total=   2.1s\n",
            "[CV] warm_start=True, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=True, total=   2.2s\n",
            "[CV] warm_start=True, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=True, total=   2.1s\n",
            "[CV] warm_start=True, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=True \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=False, criterion=gini, class_weight=balanced_subsample, bootstrap=True, total=   2.1s\n",
            "[CV] warm_start=False, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=True, total=   2.6s\n",
            "[CV] warm_start=False, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=True, total=   2.6s\n",
            "[CV] warm_start=False, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=True, total=   2.7s\n",
            "[CV] warm_start=False, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=True, total=   2.6s\n",
            "[CV] warm_start=False, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=True \n",
            "[CV]  warm_start=False, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=True, total=   2.5s\n",
            "[CV] warm_start=False, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=False \n",
            "[CV]  warm_start=False, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=False, total=   3.9s\n",
            "[CV] warm_start=False, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=False \n",
            "[CV]  warm_start=False, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=False, total=   3.8s\n",
            "[CV] warm_start=False, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=False \n",
            "[CV]  warm_start=False, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=False, total=   3.9s\n",
            "[CV] warm_start=False, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=False \n",
            "[CV]  warm_start=False, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=False, total=   3.9s\n",
            "[CV] warm_start=False, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=False \n",
            "[CV]  warm_start=False, oob_score=False, criterion=entropy, class_weight=balanced, bootstrap=False, total=   3.7s\n",
            "[CV] warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced, bootstrap=False \n",
            "[CV]  warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced, bootstrap=False, total=   0.0s\n",
            "[CV] warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced, bootstrap=False \n",
            "[CV]  warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced, bootstrap=False, total=   0.0s\n",
            "[CV] warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced, bootstrap=False \n",
            "[CV]  warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced, bootstrap=False, total=   0.0s\n",
            "[CV] warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced, bootstrap=False \n",
            "[CV]  warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced, bootstrap=False, total=   0.0s\n",
            "[CV] warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced, bootstrap=False \n",
            "[CV]  warm_start=False, oob_score=True, criterion=entropy, class_weight=balanced, bootstrap=False, total=   0.0s\n",
            "[CV] warm_start=True, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=False \n",
            "[CV]  warm_start=True, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=False, total=   0.0s\n",
            "[CV] warm_start=True, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=False \n",
            "[CV]  warm_start=True, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=False, total=   0.0s\n",
            "[CV] warm_start=True, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=False \n",
            "[CV]  warm_start=True, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=False, total=   0.0s\n",
            "[CV] warm_start=True, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=False \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py:536: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
            "ValueError: Out of bag estimation only available if bootstrap=True\n",
            "\n",
            "  FitFailedWarning)\n",
            "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:  3.4min finished\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  warm_start=True, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=False, total=   0.0s\n",
            "[CV] warm_start=True, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=False \n",
            "[CV]  warm_start=True, oob_score=True, criterion=gini, class_weight=balanced, bootstrap=False, total=   0.0s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=None, error_score=nan,\n",
              "                   estimator=RandomForestClassifier(bootstrap=True,\n",
              "                                                    ccp_alpha=0.0,\n",
              "                                                    class_weight=None,\n",
              "                                                    criterion='gini',\n",
              "                                                    max_depth=None,\n",
              "                                                    max_features='auto',\n",
              "                                                    max_leaf_nodes=None,\n",
              "                                                    max_samples=None,\n",
              "                                                    min_impurity_decrease=0.0,\n",
              "                                                    min_impurity_split=None,\n",
              "                                                    min_samples_leaf=1,\n",
              "                                                    min_samples_split=2,\n",
              "                                                    min_weight_fraction_leaf=0.0,\n",
              "                                                    n_estimators=100,\n",
              "                                                    n_j...\n",
              "                   iid='deprecated', n_iter=20, n_jobs=None,\n",
              "                   param_distributions={'bootstrap': [True, False],\n",
              "                                        'class_weight': ['balanced',\n",
              "                                                         'balanced_subsample'],\n",
              "                                        'criterion': ['gini', 'entropy'],\n",
              "                                        'oob_score': [True, False],\n",
              "                                        'warm_start': [True, False]},\n",
              "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
              "                   return_train_score=False,\n",
              "                   scoring=make_scorer(recall_score, pos_label=yes), verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00056-3e242c77-f6f5-4da1-ab9a-ad92ef85a763",
        "deepnote_cell_type": "text-cell-h2",
        "id": "3z1niQOj6Ner"
      },
      "source": [
        "## Check Best Parameter using Randomized Search CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "d704bb5e",
        "execution_millis": 13,
        "output_cleared": false,
        "cell_id": "00057-50e540a3-0b8e-4a06-ad9d-1b417ea36b2a",
        "execution_start": 1618510683606,
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvTEZRjW6Ner",
        "outputId": "6f1a1b57-5f9e-48b2-ceef-7933dd12ce50"
      },
      "source": [
        "grid_model.best_params_"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bootstrap': True,\n",
              " 'class_weight': 'balanced_subsample',\n",
              " 'criterion': 'entropy',\n",
              " 'oob_score': True,\n",
              " 'warm_start': True}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00058-7edfde80-58a6-4f6e-8821-dcc83d4f4690",
        "deepnote_cell_type": "text-cell-h2",
        "id": "i3Fxf4Bc6Nes"
      },
      "source": [
        "## Input Best Parameter into the Random Forest Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "1277220b",
        "execution_millis": 1,
        "cell_id": "00059-be3c2883-3d56-4731-8898-2bf339dcb917",
        "execution_start": 1618510806801,
        "deepnote_cell_type": "code",
        "id": "-2wu4S4y6Net"
      },
      "source": [
        "model = RandomForestClassifier(class_weight='balanced_subsample',bootstrap=True,criterion='entropy',oob_score=True,warm_start=True)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00060-f1e7e750-ad87-454e-8bc9-3b0745587748",
        "deepnote_cell_type": "text-cell-h2",
        "id": "4D4n42gm6Net"
      },
      "source": [
        "## Train the Model using Training Data and Validation Training Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "21cd9296",
        "execution_millis": 4686,
        "cell_id": "00061-bd3d4550-2fa8-4028-8496-ea19094a3a1e",
        "execution_start": 1618510808194,
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQmfAV7r6Neu",
        "outputId": "1381bb41-1808-4883-95ca-176fd1998f21"
      },
      "source": [
        "model.fit(mixSample_X, mixSample_Y)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
              "                       class_weight='balanced_subsample', criterion='entropy',\n",
              "                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
              "                       max_samples=None, min_impurity_decrease=0.0,\n",
              "                       min_impurity_split=None, min_samples_leaf=1,\n",
              "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
              "                       n_estimators=100, n_jobs=None, oob_score=True,\n",
              "                       random_state=None, verbose=0, warm_start=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00062-d5f04ee9-d048-40eb-bb64-2a0b85570eae",
        "deepnote_cell_type": "text-cell-h2",
        "id": "-DXhCKCk6Nev"
      },
      "source": [
        "## Predict Model using Testing Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "48992fe",
        "execution_millis": 424,
        "cell_id": "00063-b8ca9134-a7dc-4b80-b99e-5b3d893e94d9",
        "execution_start": 1618510816637,
        "deepnote_cell_type": "code",
        "id": "D1aJyEbv6Nev"
      },
      "source": [
        "y_pred = model.predict(X_test_scaled)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00064-9974ab37-4886-4499-88e3-7791a9b7581e",
        "deepnote_cell_type": "text-cell-h2",
        "id": "NdMSLZiz6New"
      },
      "source": [
        "## Evaluation Metrics using Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "834f5afa",
        "execution_millis": 22,
        "cell_id": "00065-1b972ab9-7b8b-4871-ab53-2cbf27793cbb",
        "execution_start": 1618510818717,
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPg6mCNs6New",
        "outputId": "8e530018-3c90-4887-eaa6-c096174478cf"
      },
      "source": [
        "confusion_matrix = pd.crosstab(y_test, y_pred)\n",
        "print (confusion_matrix)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "col_0     no   yes\n",
            "y                 \n",
            "no     10767  1239\n",
            "yes      358  1200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00066-f37887e3-6786-478b-ba94-679abb58d55b",
        "deepnote_cell_type": "text-cell-p",
        "id": "EaXzIKM96Nex"
      },
      "source": [
        " The conclusion from the above result: \n",
        "- True Positive (TP): **1200** 'Yes' predicted data is correct **from 1558** 'Yes' original data \n",
        "- True Negative (TN): **10767** 'No' predicted data is correct **from 12006** 'No' original data\n",
        "- False Negative (FN): **358** 'Yes' predicted data falsely predicted as 'No' data\n",
        "- False Positive (FP): **1239** 'No' predicted data falsely predicted as 'Yes' data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00067-ed3c1784-797e-4994-8be8-69de841ff63f",
        "deepnote_cell_type": "text-cell-h2",
        "id": "SoJKItuk6Ney"
      },
      "source": [
        "## Evaluation Metrics using Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "74950708",
        "execution_millis": 1295,
        "cell_id": "00068-784c4282-3ce0-4506-b1e8-446d978b8e12",
        "execution_start": 1618510690130,
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4MSK_-o6Ney",
        "outputId": "e0a1a749-a075-448a-ac65-acca56319097"
      },
      "source": [
        "print(classification_report(y_test,y_pred))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          no       0.97      0.90      0.93     12006\n",
            "         yes       0.49      0.77      0.60      1558\n",
            "\n",
            "    accuracy                           0.88     13564\n",
            "   macro avg       0.73      0.83      0.77     13564\n",
            "weighted avg       0.91      0.88      0.89     13564\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00069-d51f8c34-7d56-46d1-b105-1b3563b7a86c",
        "deepnote_cell_type": "text-cell-p",
        "id": "874oJj6i6Nez"
      },
      "source": [
        " The conclusion from the above result:\n",
        "- Precision: **97%** 'No' predicted data are correctly predict &amp; **49%** 'Yes' predicted data are correctly predict\n",
        "- Recall: **90%** 'No' original data have been predicted by the model &amp; **77%** 'Yes' original data have been predicted by the model\n",
        "- F1-Score: The mean from precision and recall for 'No' predicted data is **93%** and for 'Yes' predicted data is **60%**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "cell_id": "00070-2665a5ab-285e-42f3-82cd-a770c2c58dfb",
        "deepnote_cell_type": "text-cell-h1",
        "id": "sPk2RsIW6Ne0"
      },
      "source": [
        "# Evaluation Metrics using Cross Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "8620789e",
        "execution_millis": 3189,
        "cell_id": "00072-f7510a4f-8c16-4b5c-b95f-685f4e7794c5",
        "execution_start": 1618509125973,
        "deepnote_cell_type": "code",
        "id": "TME_7FsJP3j_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7228a1f6-3b11-4ad2-edd5-0e58372971fa"
      },
      "source": [
        "cv = KFold(n_splits=5, shuffle=True)\n",
        "scores = cross_val_score(model, split_input_data, split_output_data, cv=cv)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "119a8f77",
        "execution_millis": 10,
        "cell_id": "00073-8dd91331-6096-4f83-9c79-a86dab828238",
        "execution_start": 1618509129167,
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nK6B3rwiP3kB",
        "outputId": "e4f1411c-9327-47ad-8fff-abea8afe37fa"
      },
      "source": [
        "print('F1_Score: %.3f (%.3f)' % (mean(scores), std(scores)))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1_Score: 0.904 (0.002)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": false,
        "source_hash": "6ae4d27b",
        "cell_id": "00074-ab6a027c-edfe-4df3-812b-41bb2a1471a7",
        "execution_start": 1618509129174,
        "execution_millis": 535,
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTu5cNVyP3kC",
        "outputId": "7ed167d4-8d0e-46c7-a044-7c748ed1ef54"
      },
      "source": [
        "kf= KFold(n_splits=5)\n",
        "X = split_input_data.to_numpy()\n",
        "y = split_output_data\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "F1=[]\n",
        "Accuracy=[]\n",
        "Recall=[]\n",
        "Precision=[]\n",
        "for fold, (train_index, test_index) in enumerate(kf.split(X), 1):\n",
        "    X_train = X[train_index]\n",
        "    y_train = y[train_index]  # Based on your code, you might need a ravel call here, but I would look into how you're generating your y\n",
        "    X_test = X[test_index]\n",
        "    y_test = y[test_index]  # See comment on ravel and  y_train\n",
        "    \n",
        "    #Create the Dataframe\n",
        "    X_train = pd.DataFrame(X_train,columns=data.drop('y',axis=1).columns)\n",
        "    X_test = pd.DataFrame(X_test,columns=data.drop('y',axis=1).columns)\n",
        "    \n",
        "\n",
        "    #Standard Scalling\n",
        "    ss = StandardScaler()\n",
        "    X_train= ss.fit_transform(X_train)\n",
        "    X_test = ss.transform(X_test)\n",
        "    \n",
        "    X_train = pd.DataFrame(X_train, columns = split_input_data.columns)\n",
        "    X_test = pd.DataFrame(X_test , columns = split_input_data.columns)\n",
        "    \n",
        "\n",
        "    # Sampling\n",
        "    over = SMOTE(sampling_strategy = 0.18)\n",
        "    under = RandomUnderSampler(sampling_strategy=0.85)\n",
        "    steps = [('o',over),('u',under)]\n",
        "    pipeline = Pipeline(steps=steps)\n",
        "    \n",
        "    X_train,y_train_s =pipeline.fit_resample(X_train,y_train)\n",
        "    \n",
        "    #Modelling\n",
        "    model1 = RandomForestClassifier(class_weight='balanced_subsample',bootstrap=True,criterion='entropy',oob_score=True,warm_start=True)\n",
        "    model1.fit(X_train,y_train_s)\n",
        "    y_pred1 = model1.predict(X_test)\n",
        "    \n",
        "    F1.append(f1_score(y_test, y_pred1))\n",
        "    Accuracy.append(accuracy_score(y_test, y_pred1))\n",
        "    Recall.append(recall_score(y_test, y_pred1))\n",
        "    Precision.append(precision_score(y_test, y_pred1))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py:569: UserWarning: class_weight presets \"balanced\" or \"balanced_subsample\" are not recommended for warm_start if the fitted data differs from the full dataset. In order to use \"balanced\" weights, use compute_class_weight (\"balanced\", classes, y). In place of y you can use a large enough sample of the full training set target to properly estimate the class frequency distributions. Pass the resulting weights as the class_weight parameter.\n",
            "  warn('class_weight presets \"balanced\" or '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "deepnote_to_be_reexecuted": true,
        "source_hash": "9b0534a3",
        "cell_id": "00075-25ca92f0-6422-4c24-8013-c8e5b0b4cfc2",
        "deepnote_cell_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ydi9XlS1P3kD",
        "outputId": "543907d4-e84d-48cd-e381-1b474a66be80"
      },
      "source": [
        "print('F1 : '+str(np.mean(F1)))\n",
        "print('Accuracy : '+str(np.mean(Accuracy)))\n",
        "print('Recall : '+str(np.mean(Recall)))\n",
        "print('Precision : '+str(np.mean(Precision)))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 : 0.37229156382820516\n",
            "Accuracy : 0.7448600034591054\n",
            "Recall : 0.8227666692633031\n",
            "Precision : 0.27179513869021854\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}